{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tcc.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ddedj0mF_93B",
        "lfMNkiqvAHB5",
        "iQy0DswrzPbr",
        "cLiKTFHru9xH",
        "bBhvQ9-sp9D6",
        "J0xzRluyqD45",
        "B6NlM8u4U5Ey",
        "in1tBVTDzTvo",
        "8QEag53eqo5G",
        "wHggOaFSq4Ae",
        "TBc2jW8qrKuR",
        "esADSnmwZ1F1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPnq2pQzAXqS"
      },
      "source": [
        "### Importações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pECS2E-izj5S"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers wptools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "calCuRawAhU5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import transformers\n",
        "import json\n",
        "import wptools\n",
        "import base64\n",
        "import sys\n",
        "import re\n",
        "import datetime\n",
        "import tabulate\n",
        "import locale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRnlB4RsZgsn"
      },
      "source": [
        "### Configurações iniciais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I_7tjBiZqo6"
      },
      "source": [
        "_ = datetime.datetime.now().astimezone(datetime.timezone(datetime.timedelta(hours=-3))).strftime('%d-%m_%H-%M')\n",
        "original_spreadsheet_path = '/content/drive/MyDrive/Unisinos/2021_1/60583 - Projeto Final II/result.xlsx'\n",
        "result_spreadsheet_path = f'/content/drive/MyDrive/Unisinos/2021_1/60583 - Projeto Final II/outputs/output_{_}.xlsx'\n",
        "# result_spreadsheet_path = original_spreadsheet_path ####################################################################\n",
        "writer = pd.ExcelWriter(result_spreadsheet_path)\n",
        "df_original = pd.read_excel(original_spreadsheet_path, sheet_name='original')\n",
        "df_original.to_excel(writer, sheet_name='original', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkyvF5t2yCqK"
      },
      "source": [
        "# Maintain the data\n",
        "df_filtered = pd.read_excel(original_spreadsheet_path, sheet_name='filtered')\n",
        "df_filtered.to_excel(writer, sheet_name='filtered', index=False, freeze_panes=(1,0))\n",
        "df_sorted = pd.read_excel(original_spreadsheet_path, sheet_name='sorted')\n",
        "df_sorted.to_excel(writer, sheet_name='sorted', index=False, freeze_panes=(1,0))\n",
        "df_nltk = pd.read_excel(original_spreadsheet_path, sheet_name='NLTK')\n",
        "df_nltk.to_excel(writer, sheet_name='NLTK', index=False, freeze_panes=(1,0))\n",
        "df_bert = pd.read_excel(original_spreadsheet_path, sheet_name='XLM-RoBERTa')\n",
        "df_bert.to_excel(writer, sheet_name='XLM-RoBERTa', index=False, freeze_panes=(1,0))\n",
        "df_categories_manual = pd.read_excel(original_spreadsheet_path, sheet_name='categories_manual')\n",
        "df_categories_manual.to_excel(writer, sheet_name='categories_manual', index=False, freeze_panes=(1,0))\n",
        "df_categories_clean = pd.read_excel(original_spreadsheet_path, sheet_name='categories_clean')\n",
        "df_categories_clean.to_excel(writer, sheet_name='categories_clean', index=False, freeze_panes=(1,0))\n",
        "df_categories = pd.read_excel(original_spreadsheet_path, sheet_name='categories')\n",
        "df_categories.to_excel(writer, sheet_name='categories', index=False, freeze_panes=(1,0))\n",
        "df_leaked_data = pd.read_excel(original_spreadsheet_path, sheet_name='leaked_data')\n",
        "df_leaked_data.to_excel(writer, sheet_name='leaked_data', index=False, freeze_panes=(1,0))\n",
        "df_citizen = pd.read_excel(original_spreadsheet_path, sheet_name='citizen')\n",
        "df_citizen.to_excel(writer, sheet_name='citizen', index=False, freeze_panes=(1,0))\n",
        "df_categories_2 = pd.read_excel(original_spreadsheet_path, sheet_name='categories_2')\n",
        "df_categories_2.to_excel(writer, sheet_name='categories_2', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddedj0mF_93B"
      },
      "source": [
        "### Limpeza das informações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJPcd8KdAVrt"
      },
      "source": [
        "def _filtering_function(row):\n",
        "  title = row['title'].lower()\n",
        "  description = row['description'].lower()\n",
        "  for _ in [\"database\", \"leak\", \"leack\"]:\n",
        "    if (_ in title): # or (_ in description):\n",
        "      for _ in [\"★\", \"system requirements\", \"hacker\", \"warranty\", \"collection\", \"pack\", \"course\"]:\n",
        "        if (_ in title) or (_ in description):\n",
        "          return False\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "df_filter = df_original.apply(_filtering_function, axis='columns')\n",
        "df_filtered = df_original[df_filter]\n",
        "df_filtered.to_excel(writer, sheet_name='filtered', index=False, freeze_panes=(1,0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz45xCiCOw4D"
      },
      "source": [
        "df_sorted = df_filtered.copy()\n",
        "df_sorted['sort_title'] = df_sorted['title'].apply(lambda x: x.lower())\n",
        "df_sorted.rename(columns={'title': 'original_title', 'sort_title': 'title'}, inplace=True)\n",
        "_c = df_sorted.columns.to_list()\n",
        "df_sorted = df_sorted[[_c[0]] + [_c[-1]] + _c[1:-1]]\n",
        "\n",
        "df_sorted.replace({\n",
        "  'title': {\n",
        "    r'^(?:\\d{4} - )?(?:\\s+?)?\\([^\\)]*?\\)(?:\\s+?)?': '', \n",
        "    r'\\s+': ' ',\n",
        "    r'^\\s': '',\n",
        "    r'\\s$': '',\n",
        "    r'\\/': ' ',\n",
        "    r',': '',\n",
        "    r'(?i)^(?:full)\\s+(?:and complete)?\\s*': ''\n",
        "  },\n",
        "  'description':{\n",
        "      r\"\\r\\n\": \"\\n\",\n",
        "      r\"(?i)\\s*(?:Why Buy from us|We promise|Refund Policy|Regards)[\\s\\S]*$\": \"\"\n",
        "  }\n",
        "}, regex=True, inplace=True)\n",
        "\n",
        "df_sorted.sort_values(['title', 'seller'], ignore_index=True, inplace=True)\n",
        "df_sorted.to_excel(writer, sheet_name='sorted', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfMNkiqvAHB5"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNC3HYLZq0Sj",
        "outputId": "b57472a4-f8bf-48ed-9fbd-ffd7632044de"
      },
      "source": [
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words']);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS518jl9GzVQ"
      },
      "source": [
        "def get_continuous_chunks(text, labels=['ORGANIZATION', 'PERSON', 'GPE']):\n",
        "  def filter_words(p):\n",
        "    r = []\n",
        "    for i in p:\n",
        "      l = i.lower()\n",
        "      if l in ['']:\n",
        "        continue\n",
        "      i = re.sub(r'(?i)(?:hacked|leacked|leack|leaked|leak|database|business|full)', '', i)\n",
        "      i = re.sub(r'(?i)\\s+', ' ', i).strip()\n",
        "      if not i:\n",
        "        continue\n",
        "      r.append(i)\n",
        "    return r\n",
        "  chunked = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))\n",
        "  prev = None\n",
        "  continuous_chunk = []\n",
        "  current_chunk = []\n",
        "  for subtree in chunked:\n",
        "    if type(subtree) == nltk.tree.Tree and subtree.label() in labels:\n",
        "      current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
        "    if current_chunk:\n",
        "      named_entity = \" \".join(current_chunk)\n",
        "      if named_entity not in continuous_chunk:\n",
        "        continuous_chunk.append(named_entity)\n",
        "        current_chunk = []\n",
        "    else:\n",
        "      continue\n",
        "  return filter_words(continuous_chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSjprIuCDr_k"
      },
      "source": [
        "df_nltk = df_sorted.copy()\n",
        "df_nltk['entities'] = df_nltk['original_title'].apply(lambda x: json.dumps(get_continuous_chunks(x)))\n",
        "df_nltk.to_excel(writer, sheet_name='NLTK', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQy0DswrzPbr"
      },
      "source": [
        "### BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFWK-hXAqLDx"
      },
      "source": [
        "nlp = transformers.pipeline('ner',\n",
        "    model=\"xlm-roberta-large-finetuned-conll03-english\",\n",
        "    tokenizer=\"xlm-roberta-large-finetuned-conll03-english\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZepgAnAqDxe"
      },
      "source": [
        "def read_organizations(response):\n",
        "  def fix_domains(entity_list):\n",
        "    r = []\n",
        "    for i in entity_list:\n",
        "      l = i.lower()\n",
        "      if l in ['database', 'forum', 'business', 'consumer', 'leacked', 'leack', 'leaked', 'leak']:\n",
        "        continue\n",
        "      if len(l) <= 3:\n",
        "        continue\n",
        "      for t in  [\"com\", \"net\", \"org\", \"tv\", \"io\", \"ly\", \"cn\", \"co\", \"za\", \"kr\", \"in\", \"club\", \"fm\"]:\n",
        "        cpos = len(t) * -1\n",
        "        if l[cpos:].lower() == t and l[(cpos - 1):cpos].lower() != '.':\n",
        "          i = (i[:cpos] + '.' + t).lower()\n",
        "      r.append(i)\n",
        "    return r\n",
        "  \n",
        "  organizations = []\n",
        "  current_token = ''\n",
        "  for token in response:\n",
        "    if 'ORG' not in token['entity']:\n",
        "      continue\n",
        "    is_initial_string = token['word'][0] == '\\u2581'\n",
        "    if is_initial_string:\n",
        "      if current_token:\n",
        "        organizations.append(current_token)\n",
        "      current_token = token['word'][1:]\n",
        "    else:\n",
        "      current_token += token['word']\n",
        "  \n",
        "  return fix_domains(list(filter(None, organizations + [current_token])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLEDvO1jzfvL"
      },
      "source": [
        "df_bert = df_sorted.copy()\n",
        "df_bert['entities'] = df_bert['original_title'].apply(lambda x: json.dumps(read_organizations(nlp(x, grouped_entities=True))))\n",
        "df_bert.to_excel(writer, sheet_name='XLM-RoBERTa2', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLiKTFHru9xH"
      },
      "source": [
        "### Categorização das organizações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nKhaD914N5F"
      },
      "source": [
        "_cache = {}\n",
        "def get_wikipedia_page_details(item):\n",
        "  print(item)\n",
        "  if item in _cache:\n",
        "    return _cache[item]\n",
        "  try:\n",
        "    _page = wptools.page(item, silent=True).get().data\n",
        "    if (not _page) or (not _page['wikidata']):\n",
        "      _cache[item] = None\n",
        "      return None\n",
        "    _result = {'title': _page['title']}\n",
        "    for i in ['industry (P452)']:\n",
        "      if i in _page['wikidata']:\n",
        "        _result[i] = _page['wikidata'][i]\n",
        "    if _result:\n",
        "      _cache[item] = _result\n",
        "      return _result\n",
        "    _cache[item] = None\n",
        "    return None\n",
        "  except (ValueError, LookupError) as e:\n",
        "    _cache[item] = None\n",
        "    return None\n",
        "def filter_entities(entity):\n",
        "  if not entity:\n",
        "    return False\n",
        "  if entity.lower() in ['database', 'forum', 'business', 'consumer']:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def join_entities_and_get_categories(row, modes=['nltk', 'bert']):\n",
        "  for mode in modes:\n",
        "    raw_entities = row['entities_' + mode]\n",
        "    if type(raw_entities) != 'list':\n",
        "      raw_entities = json.loads(raw_entities)\n",
        "    filtered_entities = list(filter(filter_entities, raw_entities))\n",
        "    if not filtered_entities:\n",
        "      continue\n",
        "    original_text = ' '.join(filtered_entities)\n",
        "    original_text = re.sub(r'(?i)(database|hacked|leaked|leacked|leak|leack)\\s*', '', original_text)\n",
        "    if len(original_text) <= 3:\n",
        "      continue\n",
        "    search_texts = [original_text]\n",
        "    if original_text[-3:].lower() == 'com' and original_text[-4:-3].lower() != '.':\n",
        "      search_texts.append(original_text[:-3] + '.com')\n",
        "    if original_text[-3:].lower() == 'net' and original_text[-4:-3].lower() != '.':\n",
        "      search_texts.append(original_text[:-3] + '.net')\n",
        "    for text in search_texts:\n",
        "      result = get_wikipedia_page_details(text)\n",
        "      if result:\n",
        "        return json.dumps(result)\n",
        "  return ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LOIdtcLm-PL"
      },
      "source": [
        "sys.exit(1)\n",
        "if False:\n",
        "  df_categories_manual = df_sorted.copy()\n",
        "  df_categories_manual['entities_nltk'] = df_nltk['entities']\n",
        "  df_categories_manual['entities_bert'] = df_bert['entities']\n",
        "  df_categories_manual['categories'] = df_categories_manual.apply(join_entities_and_get_categories, axis='columns')\n",
        "\n",
        "  _ = lambda x: ''\n",
        "  df_categories_manual = df_categories_manual.assign(org_name=_, site_url=_, delete=_, gics_industry=_,\tgics_category=_, data_category=_, additional_data=_)\n",
        "\n",
        "  df_categories_manual.to_excel(writer, sheet_name='categories_manual', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TyXKKcNOtq1"
      },
      "source": [
        "df_temp = df_categories_manual.copy()\n",
        "df_filter = df_temp.apply(lambda x: pd.isna(x['delete']), axis='columns')\n",
        "df_categories_clean = df_temp[df_filter]\n",
        "df_categories_clean = df_categories_clean.drop(columns=['delete'])\n",
        "df_categories_clean.reset_index(drop=True, inplace=True)\n",
        "df_categories_clean.to_excel(writer, sheet_name='categories_clean', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c8AAHz_Xxm8"
      },
      "source": [
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBhvQ9-sp9D6"
      },
      "source": [
        "### Identificação do número de registros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSxnByU0vHcv"
      },
      "source": [
        "rules = [\n",
        "  {\n",
        "    \"regex\": re.compile(r'(?i)(?P<full>(?P<records>(?:\\d+[,\\. ]?)?(?:\\d+[,\\. ]?)?\\d+)\\+?(?:\\r\\n|\\s)?(?P<unit>government|record|records|entries|citizens|contacts|voters|accounts|emails|users|email:pass|lines|\\(plaintext\\)))'),\n",
        "    \"unit\": 1,\n",
        "    \"fields\": ['original_title', 'description'],\n",
        "  },\n",
        "  {\n",
        "    \"regex\": re.compile(r'(?i)(?P<full>(entry|count|records|total|lines)\\s?:?(?:\\r\\n|\\s)?(?P<records>(?:\\d+[,\\. ]?)?(?:\\d+[,\\. ]?)?\\d+)\\+?(?:\\s|\\r\\n)?)'),\n",
        "    \"unit\": 1,\n",
        "    \"fields\": ['description'],\n",
        "  },\n",
        "  {\n",
        "    \"regex\": re.compile(r'(?i)(?P<full>(?P<records>(?:\\d+[,\\. ]?)?(?:\\d+[,\\. ]?)?\\d+))$'),\n",
        "    \"unit\": 1,\n",
        "    \"fields\": ['title'],\n",
        "  },\n",
        "  {\n",
        "    \"regex\": re.compile(r'(?i)(?P<full>(?P<records>(?:\\d+[,\\. ]?)?(?:\\d+[,\\. ]?)?\\d+)\\+?(?:\\r\\n|\\s)?(?P<unit>k)(?:\\r\\n|\\s|$))'),\n",
        "    \"unit\": 1000,\n",
        "    \"fields\": ['original_title', 'description'],\n",
        "  },\n",
        "  {\n",
        "    \"regex\": re.compile(r'(?i)(?P<full>(?P<records>(?:\\d+[,\\. ]?)?(?:\\d+[,\\. ]?)?\\d+)\\+?(?:\\r\\n|\\s)?(?P<unit>million|m)(?:\\+|\\r\\n|\\s|$))'),\n",
        "    \"unit\": 1000000,\n",
        "    \"fields\": ['original_title', 'description'],\n",
        "  },\n",
        "]\n",
        "\n",
        "def count_records(row):\n",
        "  for rule in rules:\n",
        "    regex = rule[\"regex\"]\n",
        "    unit = rule[\"unit\"]\n",
        "    for field in rule[\"fields\"]:\n",
        "      text = row[field].strip()\n",
        "      match = regex.search(text)\n",
        "\n",
        "      if not match:\n",
        "        continue\n",
        "\n",
        "      records = match.group('records')\n",
        "      records = records.strip('\\n\\r ').replace(' ', '')\n",
        "\n",
        "      if records in ['2019', '2020', '2021']:\n",
        "        continue\n",
        "\n",
        "      if unit == 1:\n",
        "        records = records.replace(',', '').replace('.', '')\n",
        "        if float(records) < 100:\n",
        "          continue\n",
        "      else:\n",
        "        records = records.replace(',', '.')\n",
        "        dot_count = records.count('.')\n",
        "        records = records.replace('.', '', dot_count - 1)\n",
        "      return float(records) * unit\n",
        "  return 0\n",
        "\n",
        "df_categories = df_categories_clean.copy()\n",
        "\n",
        "df_categories['records'] = df_categories.apply(count_records, axis='columns')\n",
        "\n",
        "df_categories.to_excel(writer, sheet_name='categories', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0xzRluyqD45"
      },
      "source": [
        "### Identificação dos dados vazados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJAY1fQyJe-R"
      },
      "source": [
        "_regex = {\n",
        "    'email':               [re.compile(r\"(?mi)(email|e\\s?mail\\saddress|e\\saddress)\")],\n",
        "    'username':            [re.compile(r\"(?mi)(username)\")],\n",
        "    'name':                [re.compile(r\"(?mi)(\\bnames?\\b)\")],\n",
        "    'password':            [re.compile(r\"(?mi)(pass|password)s?\")],\n",
        "    'salt':                [re.compile(r\"(?mi)(salt)s?\")],\n",
        "    'password_hint':       [re.compile(r\"(?mi)(password\\shint|security\\squestions)\")],\n",
        "    '2fa':                 [re.compile(r\"(?mi)(2fa)\")],\n",
        "    'plaintext':           [re.compile(r\"(?mi)(plaintext|plain)\")],\n",
        "    'hashed':              [re.compile(r\"(?mi)(hash|md5|sha\\d|bcrypt|ipb)\")],\n",
        "    'phone':               [re.compile(r\"(?mi)((?<!i)phone)\")],\n",
        "    'ip':                  [re.compile(r\"(?mi)(\\bip\\b|ipaddress)\")],\n",
        "    'physical_address':    [re.compile(r\"(?mi)(physical|location)s?\")],\n",
        "    'geolocation':         [re.compile(r\"(?mi)(geographic\\slocations)s?\")],\n",
        "    'payment':             [re.compile(r\"(?mi)(payment)\\s(histor(y|ies)|methods?)\")],\n",
        "    'gender':              [re.compile(r\"(?mi)(gender)s?\")],\n",
        "    'dob':                 [re.compile(r\"(?mi)(dob|birth)\")],\n",
        "    'device_info':         [re.compile(r\"(?mi)(device\\sinformation|device\\sid|ipad|iphone|ipod)s?\")],\n",
        "    'site_activity':       [re.compile(r\"(?mi)(forum|site)\\s(activity|usage)\")],\n",
        "    'registration_date':   [re.compile(r\"(?mi)(registration\\sdate)\")],\n",
        "    'voter_ids':           [re.compile(r\"(?mi)(voter\\sid)s?\")],\n",
        "    'citizen_status':      [re.compile(r\"(?mi)(citizen\\sstatus)\")],\n",
        "    'contacts':            [re.compile(r\"(?mi)(contacts)\")],\n",
        "    'sexual_orientations': [re.compile(r\"(?mi)(sexual\\sorientation)s?\")],\n",
        "    'credit_card':         [re.compile(r\"(?mi)(credit\\scard)s?\")],\n",
        "    'spoken_languages':    [re.compile(r\"(?mi)(spoken\\slanguage)s?\")],\n",
        "    'survey_results':      [re.compile(r\"(?mi)(survey\\sresults)\")],\n",
        "    'mesenger':            [re.compile(r\"(?mi)(instant\\sidentit(?:y|ies)|instant\\smessenger\\sidentit(?:y|ies)|private\\smessages?)\")],\n",
        "    'balances':            [re.compile(r\"(?mi)(account\\sbalance)s?\")],\n",
        "    'employers':           [re.compile(r\"(?mi)(employers)\")],\n",
        "    'passport':            [re.compile(r\"(?mi)(passport)s?\")],\n",
        "}\n",
        "\n",
        "df_leaked_data = df_categories.copy()\n",
        "\n",
        "def _has(row, list_of_regex):\n",
        "  for i in list_of_regex:\n",
        "    if i.search(row['description']):\n",
        "      return 1\n",
        "  return 0\n",
        "\n",
        "for i in _regex:\n",
        "  df_leaked_data[i] = df_leaked_data.apply(lambda x: _has(x, _regex[i]), axis='columns')\n",
        "\n",
        "df_leaked_data.to_excel(writer, sheet_name='leaked_data', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6NlM8u4U5Ey"
      },
      "source": [
        "### Citizen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8iq_YSaU7HW"
      },
      "source": [
        "sys.exit(1)\n",
        "if False:\n",
        "  temp_df = df_leaked_data.copy()\n",
        "  df_citizen = temp_df.loc[df_leaked_data.data_category == 'citizen']\n",
        "  _ = lambda x: ''\n",
        "  df_citizen = df_citizen.assign(country=_, state=_)\n",
        "  df_citizen.to_excel(writer, sheet_name='citizen', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in1tBVTDzTvo"
      },
      "source": [
        "### Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XKecsgLgUjR"
      },
      "source": [
        "def format_table(text):\n",
        "  return re.sub(r' {2,}', ' ', text.replace('\\\\toprule', '\\\\hline').replace('\\\\midrule', '\\\\hline').replace('\\\\bottomrule', '\\\\hline'))\n",
        "\n",
        "def real_br_money_mask(my_value):\n",
        "    a = '{:,.2f}'.format(float(my_value))\n",
        "    b = a.replace(',','v')\n",
        "    c = b.replace('.',',')\n",
        "    return c.replace('v','.')\n",
        "  \n",
        "filter_all_df = (~df_leaked_data['data_category'].isin(['?', 'unknow'])) & (df_leaked_data['gics_industry'] != 'unknow')\n",
        "all_df = df_leaked_data[filter_all_df]\n",
        "citizen_df = df_citizen\n",
        "filter_org_df = ~all_df['data_category'].isin(['citizen', 'combo', 'phone', 'email'])\n",
        "org_df = all_df[filter_org_df]\n",
        "accuracy_df = df_categories_2[filter_all_df][filter_org_df]\n",
        "all_df.to_excel(writer, sheet_name='final_all_df', index=False, freeze_panes=(1,0))\n",
        "citizen_df.to_excel(writer, sheet_name='final_citizen_df', index=False, freeze_panes=(1,0))\n",
        "org_df.to_excel(writer, sheet_name='final_org_df', index=False, freeze_panes=(1,0))\n",
        "accuracy_df.to_excel(writer, sheet_name='final_accuracy_df', index=False, freeze_panes=(1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QEag53eqo5G"
      },
      "source": [
        "### Vendedores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CfcO6SrR4hQ"
      },
      "source": [
        "# Vendedores\n",
        "temp_df = []\n",
        "for seller in org_df['seller'].unique():\n",
        "  row = []\n",
        "  row.append(seller)\n",
        "  d = org_df[org_df['seller'] == seller]\n",
        "  row.append(len(d))\n",
        "  row.append(len(d['site_slug'].unique()))\n",
        "  row.append(len(d['gics_industry'].unique()))\n",
        "  row.append(d['views'].sum())\n",
        "  # if len(d) == 0:\n",
        "  #   continue\n",
        "  # d = d.describe(percentiles=[.5])\n",
        "  # row.append(int(d['count']))\n",
        "  # row.append(int(d['min']))\n",
        "  # row.append(int(d['50%']))\n",
        "  # row.append(int(d['max']))\n",
        "  temp_df.append(row)  \n",
        "\n",
        "#temp_df = []\n",
        "df_sellers = pd.DataFrame(temp_df, columns=['Vendedor', 'Qtde. anúncios', 'Qtde. marketplaces', 'Qtde. setores', 'Visualizações'])\n",
        "df_sellers.sort_values(by=df_sellers.columns[1], inplace=True, ignore_index=True, ascending=False)\n",
        "format_table(df_sellers.to_latex(\n",
        "  index=False, \n",
        "  caption='Número de anúncios por vendedor.', \n",
        "  label='tab:top10sellers', \n",
        "  column_format=\"l|c|c|c\",\n",
        "  float_format=\"%.2f\",\n",
        "  header=['\\\\textbf{%s}' % i for i in df_sellers.columns.values],\n",
        "  escape=False\n",
        "))\n",
        "#display(df_sellers)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIm19Yakqwio"
      },
      "source": [
        "### NLTK vs XLM-RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rc26quC7BG8"
      },
      "source": [
        "# Assertividade do NLTK e do BERT\n",
        "temp_df = []\n",
        "\n",
        "tp1 = len(accuracy_df[(accuracy_df['avaliacao_nltk'] == 'TP') & (accuracy_df['avaliacao_bert'] != 'TP')])\n",
        "fp1 = len(accuracy_df[(accuracy_df['avaliacao_nltk'] == 'FP') & (accuracy_df['avaliacao_bert'] != 'FP')])\n",
        "tn1 = len(accuracy_df[(accuracy_df['avaliacao_nltk'] == 'TN') & (accuracy_df['avaliacao_bert'] != 'TN')])\n",
        "fn1 = len(accuracy_df[(accuracy_df['avaliacao_nltk'] == 'FN') & (accuracy_df['avaliacao_bert'] != 'FN')])\n",
        "\n",
        "tp2 = len(accuracy_df[(accuracy_df['avaliacao_bert'] == 'TP') & (accuracy_df['avaliacao_nltk'] != 'TP')])\n",
        "fp2 = len(accuracy_df[(accuracy_df['avaliacao_bert'] == 'FP') & (accuracy_df['avaliacao_nltk'] != 'FP')])\n",
        "tn2 = len(accuracy_df[(accuracy_df['avaliacao_bert'] == 'TN') & (accuracy_df['avaliacao_nltk'] != 'TN')])\n",
        "fn2 = len(accuracy_df[(accuracy_df['avaliacao_bert'] == 'FN') & (accuracy_df['avaliacao_nltk'] != 'FN')])\n",
        "\n",
        "tp = tp1 + tp2\n",
        "fp = max(fp1, fp2)\n",
        "tn = tn1 + tn2\n",
        "fn = max(fn1, fn2)\n",
        "\n",
        "print(f'tp = {tp}\\nfp = {fp}\\ntn = {tn}\\nfn = {fn}\\n\\n')\n",
        "precision = (tp/(tp + fp))\n",
        "recall =  (tp/(tp + fn))\n",
        "f1 = ((2 * precision * recall)/(precision + recall))\n",
        "accuracy = ((tn + tp)/(tn + fp + fn + tp))\n",
        "print(accuracy, precision, recall, f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gre9ugrISedn"
      },
      "source": [
        "### Indústria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO-fOkwGf1cK"
      },
      "source": [
        "# Setores da indústria mais afetados\n",
        "temp_series = org_df.value_counts(['gics_industry']).head(10)\n",
        "total = len(org_df)\n",
        "temp_df = []\n",
        "for industry, count in temp_series.iteritems():\n",
        "  row = []\n",
        "  row.append(industry[0].replace('&', '\\\\&'))\n",
        "  row.append(count)\n",
        "  row.append(real_br_money_mask((count * 100) / total))\n",
        "  row.append(real_br_money_mask(org_df[org_df['gics_industry'] == industry[0]]['views'].sum())[:-3])\n",
        "  d = org_df[(org_df['gics_industry'] == industry[0]) & (org_df['records'] != 0)]['records']\n",
        "  if len(d) == 0:\n",
        "    i = \"-\"\n",
        "    row.append(i)\n",
        "  else:\n",
        "    i = d.sum() / len(d) / 1000000\n",
        "    row.append(real_br_money_mask(i))\n",
        "\n",
        "  temp_df.append(row)\n",
        "\n",
        "df_per_industry = pd.DataFrame(temp_df, columns=['Setor da indústria', 'Número de registros', 'Porcentagem do total (\\%)', 'Visualizações', 'Média registros'])\n",
        "df_per_industry.sort_values(by=df_per_industry.columns[1], ascending=False, inplace=True)\n",
        "df_per_industry.reset_index(drop=True)\n",
        "format_table(df_per_industry.to_latex(\n",
        "  index=False, \n",
        "  caption='Setores da indústria com mais vazamentos', \n",
        "  label='tab:top10setores', \n",
        "  column_format=\"l|c|c\",\n",
        "  float_format=\"%.2f\",\n",
        "  header=['\\\\textbf{%s}' % i for i in df_per_industry.columns.values],\n",
        "  escape=False\n",
        "))\n",
        "#df_per_industry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkl5yjHDPXXW"
      },
      "source": [
        "# Variação de registros por categoria\n",
        "temp_df = []\n",
        "for category in org_df['gics_industry'].unique():\n",
        "  row = []\n",
        "  row.append(category.replace('&', '\\\\&'))\n",
        "  d = org_df[(org_df['gics_industry'] == category) & (org_df['records'] != 0)]['records']\n",
        "  if len(d) == 0:\n",
        "    continue\n",
        "  d = d.describe(percentiles=[.5])\n",
        "  row.append(int(d['count']))\n",
        "  row.append(int(d['min']))\n",
        "  row.append(int(d['50%']))\n",
        "  row.append(int(d['max']))\n",
        "  temp_df.append(row)  \n",
        "\n",
        "df_records_per_idustry = pd.DataFrame(temp_df, columns=['Indústria', 'Qtde. anúncios', 'Mínimo', 'Mediana', 'Máximo'])\n",
        "df_records_per_idustry.sort_values(by=df_records_per_idustry.columns[4], inplace=True, ignore_index=True, ascending=False)\n",
        "df_records_per_idustry.drop(columns=[df_records_per_idustry.columns[1]], inplace=True)\n",
        "format_table(df_records_per_idustry.head(10).to_latex(\n",
        "  index=False, \n",
        "  caption='Número de registros por vazamentos e indústria', \n",
        "  label='tab:top10recordsperindustry', \n",
        "  column_format=\"l|c|c|c\",\n",
        "  float_format=\"%.2f\",\n",
        "  header=['\\\\textbf{%s}' % i for i in df_records_per_idustry.columns.values],\n",
        "  escape=False\n",
        "))\n",
        "display(df_records_per_idustry)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JnjRyrB4RRn"
      },
      "source": [
        "# Views por categoria\n",
        "temp_df = []\n",
        "for category in org_df['gics_industry'].unique():\n",
        "  row = []\n",
        "  row.append(category.replace('&', '\\\\&'))\n",
        "  d = org_df[(org_df['gics_industry'] == category) & (org_df['views'] != 0)]['views']\n",
        "  if len(d) == 0:\n",
        "    continue\n",
        "  sum = d.sum()\n",
        "  d = d.describe(percentiles=[.5])\n",
        "  row.append(int(d['count']))\n",
        "  row.append(int(d['min']))\n",
        "  row.append(int(d['50%']))\n",
        "  row.append(int(d['max']))\n",
        "  row.append(int(sum))\n",
        "  temp_df.append(row)  \n",
        "\n",
        "df_views_per_idustry = pd.DataFrame(temp_df, columns=['Indústria', 'Qtde. anúncios', 'Mínimo', 'Mediana', 'Máximo', 'Soma'])\n",
        "df_views_per_idustry.sort_values(by=df_views_per_idustry.columns[3], inplace=True, ignore_index=True, ascending=False)\n",
        "df_views_per_idustry.drop(columns=[df_views_per_idustry.columns[1]], inplace=True)\n",
        "format_table(df_views_per_idustry.head(10).to_latex(\n",
        "  index=False, \n",
        "  caption='Número de views por anúncio e indústria', \n",
        "  label='tab:top10viewsperindustry', \n",
        "  column_format=\"l|c|c|c\",\n",
        "  float_format=\"%.2f\",\n",
        "  header=['\\\\textbf{%s}' % i for i in df_views_per_idustry.columns.values],\n",
        "  escape=False\n",
        "))\n",
        "display(df_views_per_idustry.head(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPxS1Nl9mhxu"
      },
      "source": [
        "# Variação de preço por categoria\n",
        "temp_df = []\n",
        "for category in org_df['gics_industry'].unique():\n",
        "  row = []\n",
        "  row.append(category.replace('&', '\\\\&'))\n",
        "  z = org_df[org_df['gics_industry'] == category]\n",
        "  price_per_record = (org_df[(org_df['gics_industry'] == category) & (org_df['records'] != 0)]['price'].sum() / org_df[(org_df['gics_industry'] == category) & (org_df['records'] != 0)]['records'].sum()) *1000000\n",
        "  d = z['price'].describe(percentiles=[.5,.75, .85, .90, .95])\n",
        "  row.append(int(d['count']))\n",
        "  row.append(real_br_money_mask(d['min']))\n",
        "  row.append(real_br_money_mask(d['50%']))\n",
        "  row.append(real_br_money_mask(d['80%']))\n",
        "  row.append(real_br_money_mask(d['max']))\n",
        "  row.append(real_br_money_mask(price_per_record))\n",
        "  temp_df.append(row)  \n",
        "\n",
        "df_price_per_idustry = pd.DataFrame(temp_df, columns=['Indústria', 'Qtde. anúncios', 'Mínimo', 'Mediana', '75\\%', 'Máximo', 'Valor por registro'])\n",
        "df_price_per_idustry.sort_values(by=df_price_per_idustry.columns[1], inplace=True, ignore_index=True, ascending=False)\n",
        "df_price_per_idustry.drop(columns=[df_price_per_idustry.columns[1]], inplace=True)\n",
        "format_table(df_price_per_idustry.head(10).to_latex(\n",
        "  index=False, \n",
        "  caption='Valores dos vazamentos por indústria (Valores em USD)', \n",
        "  label='tab:top10priceperindustry', \n",
        "  column_format=\"l|r|r|r|r|r|r|r\",\n",
        "  float_format=\"%.2f\",\n",
        "  header=['\\\\textbf{%s}' % i for i in df_price_per_idustry.columns.values],\n",
        "  escape=False\n",
        "))\n",
        "display(df_price_per_idustry.head(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHggOaFSq4Ae"
      },
      "source": [
        "### Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2S7dzlefzXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49884028-f728-4b18-f00b-f07ec6cbb687"
      },
      "source": [
        "# Dados mais vazados\n",
        "_regex={'email':[re.compile('(?mi)(email|e\\\\s?mail\\\\saddress|e\\\\saddress)')],'username':[re.compile('(?mi)(username)')],'name':[re.compile('(?mi)(\\\\bnames?\\\\b)')],'password':[re.compile('(?mi)(pass|password)s?')],'salt':[re.compile('(?mi)(salt)s?')],'password_hint':[re.compile('(?mi)(password\\\\shint|security\\\\squestions)')],'2fa':[re.compile('(?mi)(2fa)')],'plaintext':[re.compile('(?mi)(plaintext|plain)')],'hashed':[re.compile('(?mi)(hash|md5|sha\\\\d|bcrypt|ipb)')],'phone':[re.compile('(?mi)((?<!i)phone)')],'ip':[re.compile('(?mi)(\\\\bip\\\\b|ipaddress)')],'physical_address':[re.compile('(?mi)(physical|location)s?')],'geolocation':[re.compile('(?mi)(geographic\\\\slocations)s?')],'payment':[re.compile('(?mi)(payment)\\\\s(histor(y|ies)|methods?)')],'gender':[re.compile('(?mi)(gender)s?')],'dob':[re.compile('(?mi)(dob|birth)')],'device_info':[re.compile('(?mi)(device\\\\sinformation|device\\\\sid|ipad|iphone|ipod)s?')],'site_activity':[re.compile('(?mi)(forum|site)\\\\s(activity|usage)')],'registration_date':[re.compile('(?mi)(registration\\\\sdate)')],'voter_ids':[re.compile('(?mi)(voter\\\\sid)s?')],'citizen_status':[re.compile('(?mi)(citizen\\\\sstatus)')],'contacts':[re.compile('(?mi)(contacts)')],'sexual_orientations':[re.compile('(?mi)(sexual\\\\sorientation)s?')],'credit_card':[re.compile('(?mi)(credit\\\\scard)s?')],'spoken_languages':[re.compile('(?mi)(spoken\\\\slanguage)s?')],'survey_results':[re.compile('(?mi)(survey\\\\sresults)')],'mesenger':[re.compile('(?mi)(instant\\\\sidentit(?:y|ies)|instant\\\\smessenger\\\\sidentit(?:y|ies)|private\\\\smessages?)')],'balances':[re.compile('(?mi)(account\\\\sbalance)s?')],'employers':[re.compile('(?mi)(employers)')],'passport':[re.compile('(?mi)(passport)s?')]}\n",
        "fields_translation = {'email': 'E-mail', 'username': 'Nome de usuário', 'name': 'Nome', 'password': 'Senha', 'salt': 'Complemento de senha', 'password_hint': 'Dica de senha', '2fa': 'Autenticação de 2 fatores', 'plaintext': 'Texto plano', 'hashed': 'Texto codificado', 'phone': 'Número de telefone', 'ip': 'Endereço IP', 'physical_address': 'Endereço', 'geolocation': 'Geolocalização', 'payment': 'Dados de pagamento', 'gender': 'Gênero', 'dob': 'Data de nascimento', 'device_info': 'Dados de dispositivos', 'site_activity': 'Atividade do site', 'registration_date': 'Data de registro', 'voter_ids': 'Documento de eleitor', 'citizen_status': 'Status de cidadão', 'contacts': 'Contatos', 'sexual_orientations': 'Orientação sexual', 'credit_card': 'Cartão de crédito', 'spoken_languages': 'Idiomas', 'survey_results': 'Resultados de pesquisa', 'mesenger': 'Mensagens', 'balances': 'Balanço das contas', 'employers': 'Empregadores', 'passport': 'Passaporte'}\n",
        "temp_df = []\n",
        "\n",
        "def verify_if_identified(row):\n",
        "  for key in _regex.keys():\n",
        "    if int(row[key]) != 0:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "\n",
        "len_identified = len(org_df[org_df.apply(verify_if_identified, axis='columns')])\n",
        "len_org_df = len(org_df)\n",
        "for field in _regex:\n",
        "  row = []\n",
        "  row.append(fields_translation[field])\n",
        "  sum = org_df[field].sum()\n",
        "  row.append(sum)\n",
        "  row.append(real_br_money_mask(sum)[:-3])\n",
        "  row.append(real_br_money_mask(sum * 100 / len_identified))\n",
        "  row.append(real_br_money_mask(sum * 100 / len_org_df))\n",
        "  row.append(real_br_money_mask(org_df[(org_df[field] == 1) & (org_df['views'] != 0)]['views'].sum())[:-3])\n",
        "  temp_df.append(row)\n",
        "\n",
        "df_leaked_data_table = pd.DataFrame(temp_df, columns=['Tipo de informação', 'registro', 'Qtde. registros', 'Identificados (\\%)', 'Total (\\%)', 'Visualizações'])\n",
        "df_leaked_data_table.sort_values(by=df_leaked_data_table.columns[1], ascending=False, inplace=True)\n",
        "df_leaked_data_table.drop(columns=[df_leaked_data_table.columns[1]], inplace=True)\n",
        "df_leaked_data_table.reset_index(drop=True)\n",
        "format_table(df_leaked_data_table.head(10).to_latex(\n",
        "  index=False, \n",
        "  caption='Top 10 tipos de informações', \n",
        "  label='tab:top10infos', \n",
        "  column_format=\"l|c|c|c\",\n",
        "  float_format=\"%.2f\",\n",
        "  header=['\\\\textbf{%s}' % i for i in df_leaked_data_table.columns.values],\n",
        "  escape=False\n",
        "))\n",
        "#display(df_leaked_data_table)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\\\begin{table}\\n\\\\centering\\n\\\\caption{Top 10 tipos de informações}\\n\\\\label{tab:top10infos}\\n\\\\begin{tabular}{l|c|c|c}\\n\\\\hline\\n\\\\textbf{Tipo de informação} & \\\\textbf{Qtde. registros} & \\\\textbf{Identificados (\\\\%)} & \\\\textbf{Total (\\\\%)} & \\\\textbf{Visualizações} \\\\\\\\\\n\\\\hline\\n Senha & 868 & 77,71 & 65,91 & 85.373 \\\\\\\\\\n E-mail & 858 & 76,81 & 65,15 & 84.849 \\\\\\\\\\n Texto plano & 559 & 50,04 & 42,44 & 56.920 \\\\\\\\\\n Nome de usuário & 559 & 50,04 & 42,44 & 53.373 \\\\\\\\\\n Texto codificado & 344 & 30,80 & 26,12 & 32.741 \\\\\\\\\\n Endereço IP & 309 & 27,66 & 23,46 & 29.622 \\\\\\\\\\n Nome & 209 & 18,71 & 15,87 & 20.104 \\\\\\\\\\n Data de nascimento & 182 & 16,29 & 13,82 & 18.311 \\\\\\\\\\n Número de telefone & 142 & 12,71 & 10,78 & 15.099 \\\\\\\\\\n Endereço & 139 & 12,44 & 10,55 & 13.751 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{table}\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cPnAyj03T41"
      },
      "source": [
        "# Views por dado vazado\n",
        "_regex={'email':[re.compile('(?mi)(email|e\\\\s?mail\\\\saddress|e\\\\saddress)')],'username':[re.compile('(?mi)(username)')],'name':[re.compile('(?mi)(\\\\bnames?\\\\b)')],'password':[re.compile('(?mi)(pass|password)s?')],'salt':[re.compile('(?mi)(salt)s?')],'password_hint':[re.compile('(?mi)(password\\\\shint|security\\\\squestions)')],'2fa':[re.compile('(?mi)(2fa)')],'plaintext':[re.compile('(?mi)(plaintext|plain)')],'hashed':[re.compile('(?mi)(hash|md5|sha\\\\d|bcrypt|ipb)')],'phone':[re.compile('(?mi)((?<!i)phone)')],'ip':[re.compile('(?mi)(\\\\bip\\\\b|ipaddress)')],'physical_address':[re.compile('(?mi)(physical|location)s?')],'geolocation':[re.compile('(?mi)(geographic\\\\slocations)s?')],'payment':[re.compile('(?mi)(payment)\\\\s(histor(y|ies)|methods?)')],'gender':[re.compile('(?mi)(gender)s?')],'dob':[re.compile('(?mi)(dob|birth)')],'device_info':[re.compile('(?mi)(device\\\\sinformation|device\\\\sid|ipad|iphone|ipod)s?')],'site_activity':[re.compile('(?mi)(forum|site)\\\\s(activity|usage)')],'registration_date':[re.compile('(?mi)(registration\\\\sdate)')],'voter_ids':[re.compile('(?mi)(voter\\\\sid)s?')],'citizen_status':[re.compile('(?mi)(citizen\\\\sstatus)')],'contacts':[re.compile('(?mi)(contacts)')],'sexual_orientations':[re.compile('(?mi)(sexual\\\\sorientation)s?')],'credit_card':[re.compile('(?mi)(credit\\\\scard)s?')],'spoken_languages':[re.compile('(?mi)(spoken\\\\slanguage)s?')],'survey_results':[re.compile('(?mi)(survey\\\\sresults)')],'mesenger':[re.compile('(?mi)(instant\\\\sidentit(?:y|ies)|instant\\\\smessenger\\\\sidentit(?:y|ies)|private\\\\smessages?)')],'balances':[re.compile('(?mi)(account\\\\sbalance)s?')],'employers':[re.compile('(?mi)(employers)')],'passport':[re.compile('(?mi)(passport)s?')]}\n",
        "temp_df = []\n",
        "# display(org_df)\n",
        "for datatype in _regex:\n",
        "  row = []\n",
        "  row.append(datatype)\n",
        "  d = org_df[(org_df[datatype] == 1) & (org_df['views'] != 0)]['views']\n",
        "  if len(d) == 0:\n",
        "    continue\n",
        "  sum = d.sum()\n",
        "  d = d.describe(percentiles=[.5])\n",
        "  row.append(int(d['count']))\n",
        "  row.append(int(d['min']))\n",
        "  row.append(int(d['50%']))\n",
        "  row.append(int(d['max']))\n",
        "  row.append(int(sum))\n",
        "  temp_df.append(row)  \n",
        "\n",
        "df_views_per_datatype = pd.DataFrame(temp_df, columns=['Tipo de dados', 'Qtde. anúncios', 'Mínimo', 'Mediana', 'Máximo', 'Soma'])\n",
        "df_views_per_datatype.sort_values(by=df_views_per_datatype.columns[5], inplace=True, ignore_index=True, ascending=False)\n",
        "df_views_per_datatype.drop(columns=[df_views_per_datatype.columns[1]], inplace=True)\n",
        "# format_table(df_views_per_datatype.head(10).to_latex(\n",
        "#   index=False, \n",
        "#   caption='Número de views por anúncio e indústria', \n",
        "#   label='tab:top10viewsperdatatype', \n",
        "#   column_format=\"l|c|c|c\",\n",
        "#   float_format=\"%.2f\",\n",
        "#   header=['\\\\textbf{%s}' % i for i in df_views_per_datatype.columns.values],\n",
        "#   escape=False\n",
        "# ))\n",
        "display(df_views_per_datatype.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBc2jW8qrKuR"
      },
      "source": [
        "### Cidadãos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nnKTk3UTSAL"
      },
      "source": [
        "# Cidadãos\n",
        "temp_df = []\n",
        "paises = {'United Kingdom': 'Reino Unido', 'United States': 'Estados Unidos', 'Australia': 'Austrália', 'Brazil': 'Brasil', 'Bulgaria': 'Bulgária', 'Canada': 'Canadá', 'China': 'China', 'Mexico': 'México', 'Philippines': 'Filipinas', 'Russia': 'Rússia', 'Taiwan': 'Taiwan', 'Turkey': 'Turquia'}\n",
        "for country in citizen_df['country'].unique():\n",
        "  row = []\n",
        "  row.append(paises[country])\n",
        "  d = citizen_df[(citizen_df['country'] == country) & (citizen_df['records'] != 0)]\n",
        "  if len(d) == 0:\n",
        "    continue\n",
        "  row.append(len(d))\n",
        "  row.append(int(d['records'].describe()['max']))\n",
        "  temp_df.append(row)  \n",
        "\n",
        "df_records_by_country = pd.DataFrame(temp_df, columns=['País', 'Qtde. anúncios', 'Max. registros'])\n",
        "df_records_by_country.sort_values(by=df_records_by_country.columns[1], inplace=True, ignore_index=True, ascending=False)\n",
        "format_table(df_records_by_country.head(10).to_latex(\n",
        "  index=False, \n",
        "  caption='Número de registros por país', \n",
        "  label='tab:top10recordspercountry', \n",
        "  column_format=\"l|c|c\",\n",
        "  float_format=\"%.2f\",\n",
        "  header=['\\\\textbf{%s}' % i for i in df_records_by_country.columns.values],\n",
        "  escape=False\n",
        "))\n",
        "#display(df_price_per_idustry.head(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v_6RFy0g6jy"
      },
      "source": [
        "# Cidadãos - Estados EUA\n",
        "temp_df = []\n",
        "cidades = ['AL': 'ALABAMA', 'AK': 'ALASKA', 'AS': 'AMERICAN SAMOA', 'AZ': 'ARIZONA', 'AR': 'ARKANSAS', 'CA': 'CALIFORNIA', 'CO': 'COLORADO', 'CT': 'CONNECTICUT', 'DE': 'DELAWARE', 'DC': 'DISTRICT OF COLUMBIA', 'FL': 'FLORIDA', 'GA': 'GEORGIA', 'GU': 'GUAM', 'HI': 'HAWAII', 'ID': 'IDAHO', 'IL': 'ILLINOIS', 'IN': 'INDIANA', 'IA': 'IOWA', 'KS': 'KANSAS', 'KY': 'KENTUCKY', 'LA': 'LOUISIANA', 'ME': 'MAINE', 'MD': 'MARYLAND', 'MA': 'MASSACHUSETTS', 'MI': 'MICHIGAN', 'MN': 'MINNESOTA', 'MS': 'MISSISSIPPI', 'MO': 'MISSOURI', 'MT': 'MONTANA', 'NE': 'NEBRASKA', 'NV': 'NEVADA', 'NH': 'NEW HAMPSHIRE', 'NJ': 'NEW JERSEY', 'NM': 'NEW MEXICO', 'NY': 'NEW YORK', 'NC': 'NORTH CAROLINA', 'ND': 'NORTH DAKOTA', 'MP': 'NORTHERN MARIANA IS', 'OH': 'OHIO', 'OK': 'OKLAHOMA', 'OR': 'OREGON', 'PA': 'PENNSYLVANIA', 'PR': 'PUERTO RICO', 'RI': 'RHODE ISLAND', 'SC': 'SOUTH CAROLINA', 'SD': 'SOUTH DAKOTA', 'TN': 'TENNESSEE', 'TX': 'TEXAS', 'UT': 'UTAH', 'VT': 'VERMONT', 'VA': 'VIRGINIA', 'VI': 'VIRGIN ISLANDS', 'WA': 'WASHINGTON', 'WV': 'WEST VIRGINIA', 'WI': 'WISCONSIN',' 'WY': 'WYOMING']\n",
        "\n",
        "for state in citizen_df[citizen_df['country'] == 'United States']['state'].unique():\n",
        "  row = []\n",
        "  row.append(state)\n",
        "  d = citizen_df[(citizen_df['country'] == 'United States') & (citizen_df['state'] == state)]\n",
        "  # & (citizen_df['records'] != 0)\n",
        "  if len(d) == 0:\n",
        "    continue\n",
        "  row.append(len(d))\n",
        "  row.append(int(d['records'].describe()['max']))\n",
        "  temp_df.append(row)  \n",
        "\n",
        "df_records_by_country = pd.DataFrame(temp_df, columns=['Estado', 'Qtde. anúncios', 'Max. registros'])\n",
        "df_records_by_country.sort_values(by=df_records_by_country.columns[2], inplace=True, ignore_index=True, ascending=False)\n",
        "display\n",
        "display(df_records_by_country)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esADSnmwZ1F1"
      },
      "source": [
        "### Escrita da planilha e close do handler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p46fRPIZ6RV"
      },
      "source": [
        "writer.save()\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}